{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Twitter API JSON file to NetworkX & Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by Chris Allison - @ToferC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a custom JSON dict of tweets pulled from the Twitter API, extracts the key data desired and creates first a NetworkX Graph and then exports to a Gephi file.\n",
    "\n",
    "Tweets are collected by twitter_to_json.py and placed in the results directory with the title \"search_\"query_name\".json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import os\n",
    "import datetime, time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up source and data directories - configure these as required\n",
    "\n",
    "home_dir = \"/home/data/python/github/Twitter_graphing_python/results\"\n",
    "save_dir = \"/home/data/Documents/Gephi/twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your search queries here separated by spaces - \n",
    "# You must have already run search_twitter_to_json.py for the query\n",
    "\n",
    "search_queries = \"gcdigital\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used by the main script to prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_user(tweet_id, tweet_dict):\n",
    "    # Find tweet author based on tweet status ID\n",
    "    try:\n",
    "        x = tweet_dict[tweet_id]['user_screen_name']\n",
    "    except KeyError:\n",
    "        x = None # User is out of scope\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('(.)([A-Z][a-z]+)')\n",
    "all_cap_re = re.compile('([a-z0-9])([A-Z])')\n",
    "\n",
    "def convert(name):\n",
    "    # Convert text to camel_case\n",
    "    s1 = first_cap_re.sub(r'\\1_\\2', name)\n",
    "    return all_cap_re.sub(r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full function for network graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twitter_data_to_graph(search_query):\n",
    "    # Convert JSON file of tweets to NetworkX Graph and Gephi graph file\n",
    "    \n",
    "    # Set up empty strings for NTLK analysis\n",
    "    full_tweets = \"\"\n",
    "    full_hashtags = \"\"\n",
    "    \n",
    "    # Create dict for storing user data\n",
    "    \n",
    "    for sq in search_query:\n",
    "        # Open source files for each query\n",
    "        \n",
    "        fname = open(os.path.join(home_dir, 'converted_{}.json'.format(sq)))\n",
    "        \n",
    "        # Easily load JSON files with Pandas\n",
    "        df = pd.read_json(fname, convert_axes=False)\n",
    "        \n",
    "        # Convert DF to dict for easier assignment\n",
    "        tweets = df.to_dict(orient=\"dict\")\n",
    "        \n",
    "        for t in tweets:\n",
    "    \n",
    "            # Convert hashtags to string\n",
    "            n = []\n",
    "            if tweets[t]['hashtags']:\n",
    "                for i in tweets[t]['hashtags']:\n",
    "                    n.append(i.lower())\n",
    "                tweets[t]['hashtags'] = \" \".join(n)\n",
    "            else:\n",
    "                tweets[t]['hashtags'] = \"\"\n",
    "\n",
    "            # Convert user-mentions to string\n",
    "            n = []\n",
    "            if tweets[t]['user_mentions']:\n",
    "                for i in tweets[t]['user_mentions']:\n",
    "                    n.append(i.lower())\n",
    "                tweets[t]['user_mentions'] = \" \".join(n)\n",
    "            else:\n",
    "                tweets[t]['user_mentions'] = \"\"\n",
    "                \n",
    "            users = {} # Set user name to save a lot of typing\n",
    "            name = str(tweets[t]['user_screen_name']).lower()\n",
    "            \n",
    "            # User already exists\n",
    "            try:\n",
    "                # Add hashtags used by the user\n",
    "                users[name]['hashtags'] += \" {}\".format(\n",
    "                tweets[t]['hashtags'])\n",
    "\n",
    "                # Add user-mentions used by the user\n",
    "                users[name]['user_mentions'] += \" {}\".format(\n",
    "                tweets[t]['user_mentions'])\n",
    "\n",
    "                # Add tweet id_strings used by the user\n",
    "                users[name]['id_str'] += \" {}\".format(\n",
    "                tweets[t]['id_str'])\n",
    "\n",
    "                # We are not adding additional info to date as it will disrupt dynamic graphs\n",
    "                pass\n",
    "\n",
    "                # Add texts from additional tweets and date info as text\n",
    "                users[name]['text'] += \"\\n\\n{}\\n{}\".format(\n",
    "                tweets[t]['text'], tweets[t]['date'])\n",
    "\n",
    "                # Count user tweets\n",
    "                users[name]['user_tweets'] += 1\n",
    "\n",
    "            except KeyError:\n",
    "\n",
    "                # Key doesn't exist, so add user\n",
    "                users[name] = {}\n",
    "\n",
    "                users[name]['screen_name'] = name\n",
    "                users[name]['id_str'] = tweets[t]['id_str']\n",
    "                users[name]['date'] = tweets[t]['date']\n",
    "                users[name]['text'] = tweets[t]['text']\n",
    "                users[name]['hashtags'] = tweets[t]['hashtags']\n",
    "                users[name]['user_mentions'] = str(tweets[t]['user_mentions'])\n",
    "                users[name]['user_tweets'] = 1\n",
    "                \n",
    "            full_tweets += \"{} \".format(tweets[t]['text'])\n",
    "            full_hashtags += \"{} \".format(tweets[t]['hashtags'])\n",
    "                \n",
    "        # Remove duplicate hashtags\n",
    "        \n",
    "        for u in users:\n",
    "            users[u]['hashtags'] = \" \".join(set(users[u]['hashtags'].split(' ')))\n",
    "        \n",
    "        # Create one node for each tweet with embedded data - Graph N\n",
    "        for u in users:\n",
    "            N.add_node(str(users[u]['screen_name']), attr_dict=users[u])\n",
    "            \n",
    "        # Create edge_list\n",
    "\n",
    "        edge_dict = {}\n",
    "\n",
    "        for u in users:\n",
    "            temp = []\n",
    "\n",
    "            # Prep user-mentions\n",
    "            try:\n",
    "                um = users[u]['user_mentions'].split(\" \")\n",
    "            except AttributeError:\n",
    "                um = []\n",
    "\n",
    "\n",
    "            # Create edge list keys and weight\n",
    "            temp = um\n",
    "\n",
    "            # Remove any empty elements in list\n",
    "            for e in temp:\n",
    "                \n",
    "                # Remove any empty elements in list\n",
    "                if str(e) == \"\" or str(e) == 'None':\n",
    "                    pass\n",
    "                # Add edges to dict\n",
    "                else:\n",
    "                    try:\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['weight'] += 1\n",
    "\n",
    "                    except KeyError:\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()] = {}\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['node'] = users[u]['screen_name']\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['target'] = str(e)\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['weight'] = 1\n",
    "                        \n",
    "        # Add edges for @mentions and replies to users\n",
    "        for e in edge_dict:\n",
    "            N.add_edge(edge_dict[e]['node'],\n",
    "                       edge_dict[e]['target'],\n",
    "                       weight=edge_dict[e]['weight'])\n",
    "\n",
    "        # Insert any data analysis here\n",
    "        print(\"Converting and analyzing search query: {}\".format(sq))\n",
    "        print(\"Nodes: {}, Edges: {}\".format(len(N.nodes()), len(N.edges())))\n",
    "        \n",
    "        # NLTK analysis\n",
    "        \n",
    "        tweet_tokens = word_tokenize(full_tweets)\n",
    "        hashtag_tokens = word_tokenize(full_hashtags)\n",
    "        \n",
    "        text = nltk.Text(tweet_tokens)\n",
    "        clean_tweets = [w for w in set(text) if w.lower() not in stopwords.words('english')]\n",
    "        \n",
    "        hts = nltk.Text(hashtag_tokens)\n",
    "        \n",
    "        print('\\nFrequency Distribution for hashtags:')\n",
    "        fdist1 = nltk.FreqDist(hts)\n",
    "        V = fdist1.most_common(n=50)\n",
    "        print([y for y in V])\n",
    "        fdist1.plot(30, cumulative=False)\n",
    "        \n",
    "    # Write G graph in gexf for Gephi\n",
    "    file_name = \"{}_graph_{}.gexf\".format(\n",
    "        convert(\"_\".join(search_queries)),\n",
    "        datetime.datetime.now())\n",
    "    \n",
    "    nx.write_gexf(N, os.path.join(save_dir, file_name))\n",
    "    print(\"New file {} saved to {}.\".format(file_name, save_dir))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/data/python/github/Twitter_graphing_python/results/converted_gcdigital.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c2a6ea1ca9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_data_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_queries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c6ff2410d69a>\u001b[0m in \u001b[0;36mtwitter_data_to_graph\u001b[0;34m(search_query)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Open source files for each query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'converted_{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Easily load JSON files with Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/data/python/github/Twitter_graphing_python/results/converted_gcdigital.json'"
     ]
    }
   ],
   "source": [
    "twitter_data_to_graph(search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done!  Now go check out your graph file in Gephi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'@timsargent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0d4cdbbd36de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'@timsargent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '@timsargent'"
     ]
    }
   ],
   "source": [
    "users['@timsargent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
