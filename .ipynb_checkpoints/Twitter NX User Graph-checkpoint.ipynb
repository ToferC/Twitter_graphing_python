{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Twitter API JSON file to NetworkX & Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by Chris Allison - @ToferC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a custom JSON dict of tweets pulled from the Twitter API, extracts the key data desired and creates first a NetworkX Graph and then exports to a Gephi file.\n",
    "\n",
    "Tweets are collected by twitter_to_json.py and placed in the results directory with the title \"search_\"query_name\".json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (parser.py, line 158)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/home/chris/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\"\u001b[0;36m, line \u001b[0;32m158\u001b[0m\n\u001b[0;31m    l.append(\"%s=%s\" % (attr, `value`))\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import os\n",
    "import datetime, time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-622e1863a106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up source and data directories - configure these as required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhome_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/chris/Documents/Gephi/twitter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up source and data directories - configure these as required\n",
    "\n",
    "home_dir = os.path.join(os.path.home_dir, \"/results\")\n",
    "save_dir = \"/home/chris/Documents/Gephi/twitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your search queries here separated by spaces - \n",
    "# You must have already run search_twitter_to_json.py for the query\n",
    "\n",
    "search_queries = \"ashleysmith\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used by the main script to prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_user(tweet_id, tweet_dict):\n",
    "    # Find tweet author based on tweet status ID\n",
    "    try:\n",
    "        x = tweet_dict[tweet_id]['user_screen_name']\n",
    "    except KeyError:\n",
    "        x = None # User is out of scope\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_cap_re = re.compile('(.)([A-Z][a-z]+)')\n",
    "all_cap_re = re.compile('([a-z0-9])([A-Z])')\n",
    "\n",
    "def convert(name):\n",
    "    # Convert text to camel_case\n",
    "    s1 = first_cap_re.sub(r'\\1_\\2', name)\n",
    "    return all_cap_re.sub(r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full function for network graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twitter_data_to_graph(search_query):\n",
    "    # Convert JSON file of tweets to NetworkX Graph and Gephi graph file\n",
    "    \n",
    "    # Set up empty strings for NTLK analysis\n",
    "    full_tweets = \"\"\n",
    "    full_hashtags = \"\"\n",
    "    \n",
    "    # Create dict for storing user data\n",
    "    users = {}\n",
    "    \n",
    "    for sq in search_query:\n",
    "        # Open source files for each query\n",
    "        \n",
    "        fname = open(os.path.join(home_dir, 'search_{}.json'.format(sq)))\n",
    "        \n",
    "        # Easily load JSON files with Pandas\n",
    "        df = pd.read_json(fname, convert_axes=False)\n",
    "        \n",
    "        # Convert DF to dict for easier assignment\n",
    "        tweets = df.to_dict(orient=\"dict\")\n",
    "        \n",
    "        for t in tweets:\n",
    "    \n",
    "            # Convert hashtags to string\n",
    "            n = []\n",
    "            if tweets[t]['hashtags']:\n",
    "                for i in tweets[t]['hashtags']:\n",
    "                    n.append(i['text'].lower())\n",
    "                tweets[t]['hashtags'] = \" \".join(n)\n",
    "            else:\n",
    "                tweets[t]['hashtags'] = \"\"\n",
    "\n",
    "            # Convert user-mentions to string\n",
    "            n = []\n",
    "            if tweets[t]['user_mentions']:\n",
    "                for i in tweets[t]['user_mentions']:\n",
    "                    n.append(i['screen_name'].lower())\n",
    "                tweets[t]['user_mentions'] = \" \".join(n)\n",
    "            else:\n",
    "                tweets[t]['user_mentions'] = \"\"\n",
    "                \n",
    "            # Set user name to save a lot of typing\n",
    "            name = str(tweets[t]['user_screen_name']).lower()\n",
    "            \n",
    "            # User already exists\n",
    "            try:\n",
    "                # Add hashtags used by the user\n",
    "                users[name]['hashtags'] += \" {}\".format(\n",
    "                tweets[t]['hashtags'])\n",
    "\n",
    "                # Add user-mentions used by the user\n",
    "                users[name]['user_mentions'] += \" {}\".format(\n",
    "                tweets[t]['user_mentions'])\n",
    "\n",
    "                # Add tweet id_strings used by the user\n",
    "                users[name]['id_str'] += \" {}\".format(\n",
    "                tweets[t]['id_str'])\n",
    "\n",
    "                # We are not adding additional info to date as it will disrupt dynamic graphs\n",
    "                pass\n",
    "\n",
    "                # Add texts from additional tweets and date info as text\n",
    "                users[name]['text'] += \"\\n\\n{}\\n{}\".format(\n",
    "                tweets[t]['text'], tweets[t]['date'])\n",
    "\n",
    "                # Count user tweets\n",
    "                users[name]['user_tweets'] += 1\n",
    "\n",
    "                # Add cumulative retweets for all tweets\n",
    "                users[name]['retweet_count'] += int(tweets[t]['retweet_count'])\n",
    "\n",
    "                users[name]['replies'] += \" {}\".format(tweets[t]['reply_to_tweet'])\n",
    "                users[name]['quoted_id_str'] += \" {}\".format(tweets[t]['quoted_status_id_str'])\n",
    "\n",
    "                users[name]['user_replies'] += \" {}\".format(tweets[t]['reply_to'])\n",
    "\n",
    "            except KeyError:\n",
    "\n",
    "                # Key doesn't exist, so add user\n",
    "                users[name] = {}\n",
    "\n",
    "                users[name]['screen_name'] = name\n",
    "                users[name]['id_str'] = tweets[t]['id_str']\n",
    "                users[name]['date'] = tweets[t]['date']\n",
    "                users[name]['text'] = tweets[t]['text']\n",
    "                users[name]['hashtags'] = tweets[t]['hashtags']\n",
    "                users[name]['retweet_count'] = tweets[t]['retweet_count']\n",
    "                users[name]['replies'] = str(tweets[t]['reply_to_tweet'])\n",
    "                users[name]['quoted_id_str'] = str(tweets[t]['quoted_status_id_str'])\n",
    "                users[name]['user_replies'] = str(tweets[t]['reply_to'])\n",
    "                users[name]['user_mentions'] = str(tweets[t]['user_mentions'])\n",
    "                users[name]['user_tweets'] = 1\n",
    "                \n",
    "            full_tweets += \"{} \".format(tweets[t]['text'])\n",
    "            full_hashtags += \"{} \".format(tweets[t]['hashtags'])\n",
    "                \n",
    "        # Remove duplicate hashtags\n",
    "        \n",
    "        for u in users:\n",
    "            users[u]['hashtags'] = \" \".join(set(users[u]['hashtags'].split(' ')))\n",
    "        \n",
    "        # Create one node for each tweet with embedded data - Graph N\n",
    "        for u in users:\n",
    "            N.add_node(str(users[u]['screen_name']), attr_dict=users[u])\n",
    "            \n",
    "        # Create edge_list\n",
    "\n",
    "        edge_dict = {}\n",
    "\n",
    "        for u in users:\n",
    "            temp = []\n",
    "\n",
    "            # Prep user-mentions\n",
    "            try:\n",
    "                um = users[u]['user_mentions'].split(\" \")\n",
    "            except AttributeError:\n",
    "                um = []\n",
    "\n",
    "            # Prep replies to users\n",
    "            try:\n",
    "                ur = users[u]['user_replies'].split(\" \")\n",
    "            except AttributeError:\n",
    "                ur = []\n",
    "\n",
    "\n",
    "            # Prep replies to tweets\n",
    "            try:\n",
    "                rtt = users[u]['replies'].split(\" \")\n",
    "            except AttributeError:\n",
    "                rtt = []\n",
    "\n",
    "            if rtt:\n",
    "                rtt = [find_user(u, tweets) for u in rtt]\n",
    "                \n",
    "            # Prep quoted_status_id_str\n",
    "            try:\n",
    "                qis = users[u]['quoted_id_str'].split(\" \")\n",
    "            except AttributeError:\n",
    "                qis = []\n",
    "\n",
    "            if qis:\n",
    "                qis = [find_user(u, tweets) for u in qis]\n",
    "\n",
    "\n",
    "            # Create edge list keys and weight\n",
    "            temp = um + ur + rtt + qis\n",
    "\n",
    "            # Remove any empty elements in list\n",
    "            for e in temp:\n",
    "                \n",
    "                # Remove any empty elements in list\n",
    "                if str(e) == \"\" or str(e) == 'None':\n",
    "                    pass\n",
    "                # Add edges to dict\n",
    "                else:\n",
    "                    try:\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['weight'] += 1\n",
    "\n",
    "                    except KeyError:\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()] = {}\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['node'] = users[u]['screen_name']\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['target'] = str(e)\n",
    "                        edge_dict[str([users[u]['screen_name'], str(e)]).lower()]['weight'] = 1\n",
    "                        \n",
    "        # Add edges for @mentions and replies to users\n",
    "        for e in edge_dict:\n",
    "            N.add_edge(edge_dict[e]['node'],\n",
    "                       edge_dict[e]['target'],\n",
    "                       weight=edge_dict[e]['weight'])\n",
    "\n",
    "        # Insert any data analysis here\n",
    "        print(\"Converting and analyzing search query: {}\".format(sq))\n",
    "        print(\"Nodes: {}, Edges: {}\".format(len(N.nodes()), len(N.edges())))\n",
    "        \n",
    "        # NLTK analysis\n",
    "        \n",
    "        tweet_tokens = word_tokenize(full_tweets)\n",
    "        hashtag_tokens = word_tokenize(full_hashtags)\n",
    "        \n",
    "        text = nltk.Text(tweet_tokens)\n",
    "        clean_tweets = [w for w in set(text) if w.lower() not in stopwords.words('english')]\n",
    "        \n",
    "        hts = nltk.Text(hashtag_tokens)\n",
    "        \n",
    "        print('\\nFrequency Distribution for hashtags:')\n",
    "        fdist1 = nltk.FreqDist(hts)\n",
    "        V = fdist1.most_common(n=50)\n",
    "        print([y for y in V])\n",
    "        fdist1.plot(30, cumulative=False)\n",
    "        \n",
    "    # Write G graph in gexf for Gephi\n",
    "    file_name = \"{}_graph_{}.gexf\".format(\n",
    "        convert(\"_\".join(search_queries)),\n",
    "        datetime.datetime.now())\n",
    "    \n",
    "    nx.write_gexf(N, os.path.join(save_dir, file_name))\n",
    "    print(\"New file {} saved to {}.\".format(file_name, save_dir))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_data_to_graph(search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## You're done!  Now go check out your graph file in Gephi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
